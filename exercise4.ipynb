{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import mdptoolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# Switch to 2x2 flag\n",
    "# The code is written generically and originally designed for 3x3\n",
    "# with a lot of work going into solving the memory issues by sparse matrices and so on\n",
    "# but the mdp algorithms just dont terminate on my machine with 3x3\n",
    "# So in the end I decided to introduce this flag to allow evaluation\n",
    "TWO_BY_TWO = True\n",
    "\n",
    "S = 1_572_864\n",
    "A = 19\n",
    "IGNORE_ACTION = 18\n",
    "COLORS = 3\n",
    "OPERATIONS = 2 * COLORS\n",
    "STORE = 1\n",
    "COLOR_EMPTY = 3\n",
    "\n",
    "if TWO_BY_TWO:\n",
    "    A = 9\n",
    "    IGNORE_ACTION = 8\n",
    "    S = int(S / (4 ** 5))\n",
    "\n",
    "# Same props for store and restore, different props for colors\n",
    "COLOR_PROPS = np.array([.25, .15, .1, .25, .15, .1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For easier calculation the states and actions are binary mask encoded\n",
    "State Encoding\n",
    "The first 18 bits (0-17) are used for the storage state. 2 bits for each of the 9 squares.\n",
    "The 18th bit is used for the type of operation: store/restore.\n",
    "And the 19th and 20th bit to specify the color to be stored/restored.\n",
    "Action Encoding:\n",
    "The first bit specifies the type of operation: store/restore\n",
    "The following four bits specify the square to perform the operation on\n",
    "State IGNORE_ACTION is used for rejecting\n",
    "\n",
    "For 2x2:\n",
    "3 bits are enough for action encoding\n",
    "the offset for the game state changes\n",
    "basically everything changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helpers for state manipulation and read out:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def get_action(a):\n",
    "    # ACHTUNG: IGNORE_ACTION needs to be handled separately\n",
    "    action = a & 1  # first bit: store/restore\n",
    "    a = a >> 1\n",
    "\n",
    "    if not TWO_BY_TWO:\n",
    "        # Convert 0...8 to 0..2 x 0..2\n",
    "        x = a % 3\n",
    "        y = int(a / 3)\n",
    "    else:\n",
    "        x = a & 1\n",
    "        y = (a >> 1) & 1\n",
    "\n",
    "    return action, x, y\n",
    "\n",
    "def get_task(s):\n",
    "    if not TWO_BY_TWO:\n",
    "        task = s & (0b1 << 18)\n",
    "        color = s & (0b11 << 19)\n",
    "    else:\n",
    "        task = s & (0b1 << 8)\n",
    "        color = s & (0b11 << 9)\n",
    "    return task, color\n",
    "\n",
    "def get_color(s, x, y):\n",
    "    if not TWO_BY_TWO:\n",
    "        pos = 3*y + x\n",
    "    else:\n",
    "        pos = 2*y + x\n",
    "    mask = 0b11 << (pos * 2)\n",
    "    color = s & mask\n",
    "    return color\n",
    "\n",
    "\n",
    "def set_color(s, x, y, color):\n",
    "    if not TWO_BY_TWO:\n",
    "        pos = 3*y + x\n",
    "    else:\n",
    "        pos = 2*y + x\n",
    "\n",
    "    # Reset color\n",
    "    mask = 0b11 << (pos * 2)\n",
    "    s = s & (~mask)\n",
    "\n",
    "    # Set color\n",
    "    mask = color << (pos * 2)\n",
    "    s = s & mask\n",
    "\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialization of the probability matrix P."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/9\n",
      "Progress: 2/9\n",
      "Progress: 3/9\n",
      "Progress: 4/9\n",
      "Progress: 5/9\n",
      "Progress: 6/9\n",
      "Progress: 7/9\n",
      "Progress: 8/9\n",
      "Progress: 9/9\n"
     ]
    }
   ],
   "source": [
    "# number of actions sized ndarrays\n",
    "if not TWO_BY_TWO:\n",
    "    P = np.ndarray((A,), dtype=object)\n",
    "\n",
    "else:\n",
    "    P = np.zeros((A,S,S))\n",
    "\n",
    "# for every action\n",
    "for a in range(A):\n",
    "    # create a state x state sparse transition probability matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    data = []\n",
    "    # for every state\n",
    "    for s in range(S):\n",
    "        ### apply storage state change ###\n",
    "        action, x, y = get_action(a)\n",
    "\n",
    "        # Case Ignore\n",
    "        if a == IGNORE_ACTION:\n",
    "            state = s\n",
    "\n",
    "        # Case Store\n",
    "        elif action == STORE:\n",
    "            if not get_color(s, x, y) == COLOR_EMPTY:\n",
    "                state = s\n",
    "            else:\n",
    "                task, color = get_task(s)\n",
    "                state = set_color(s, x, y, color)\n",
    "        else:\n",
    "            # Case restore\n",
    "            state = set_color(s, x, y, COLOR_EMPTY)\n",
    "\n",
    "        ### get all possible state mutations for different following tasks ###\n",
    "        # Reset task in state\n",
    "        if not TWO_BY_TWO:\n",
    "            mask = ~(0b111 << 18)\n",
    "        else:\n",
    "            mask = ~(0b111 << 8)\n",
    "        state = state & mask\n",
    "\n",
    "        # get all permutations of task\n",
    "        mutations = []\n",
    "        for i in range(COLORS * 2):\n",
    "            if not TWO_BY_TWO:\n",
    "                mask = i << 18\n",
    "            else:\n",
    "                mask = i << 8\n",
    "            mutations.append(state | mask)\n",
    "\n",
    "        # add all state transition probabilities\n",
    "        for i, mut in enumerate(mutations):\n",
    "            if not TWO_BY_TWO:\n",
    "                row_indices.append(s)\n",
    "                col_indices.append(mut)\n",
    "                data.append(COLOR_PROPS[i])\n",
    "            else:\n",
    "                P[a][s][mut] = COLOR_PROPS[i]\n",
    "    if not TWO_BY_TWO:\n",
    "        P[a] = csr_matrix((data, (row_indices, col_indices)), shape=(S, S))\n",
    "    print(\"Progress: \" + str(a+1) + \"/\" + str(A))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialization of the risk matrix R."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/9\n",
      "Progress: 2/9\n",
      "Progress: 3/9\n",
      "Progress: 4/9\n",
      "Progress: 5/9\n",
      "Progress: 6/9\n",
      "Progress: 7/9\n",
      "Progress: 8/9\n",
      "Progress: 9/9\n"
     ]
    }
   ],
   "source": [
    "R = np.ndarray((S, A))\n",
    "\n",
    "# For every possible action\n",
    "for a in range(A):\n",
    "    # For every possible state\n",
    "    for s in range(S):\n",
    "\n",
    "        # extract action, x and y\n",
    "        action, x, y = get_action(a)\n",
    "\n",
    "        # extract task and color\n",
    "        task, color = get_task(s)\n",
    "\n",
    "        if a == IGNORE_ACTION:\n",
    "            R[s][a] = -10\n",
    "            continue\n",
    "\n",
    "        if not action == task: # Wrong action: illegal\n",
    "            R[s][a] = -100\n",
    "            continue\n",
    "\n",
    "        if action == STORE:\n",
    "            if not get_color(s, x, y) == COLOR_EMPTY: # Tried storing to already filled square: illegal\n",
    "                R[s][a] = -100\n",
    "                continue\n",
    "            # Else: legal storing move\n",
    "            # Penalize for distance only\n",
    "            R[s][a] = -(1+x+y)\n",
    "            continue\n",
    "\n",
    "        # Else: Action is restore\n",
    "        if get_color(s, x, y) == COLOR_EMPTY: # Nothing there to restore: illegal\n",
    "            R[s][a] = -100\n",
    "            continue\n",
    "\n",
    "        # Else: Legal Restoring\n",
    "        if not get_color(s, x, y) == color: # restored the wrong thing: illegal\n",
    "            R[s][a] = -100\n",
    "            continue\n",
    "\n",
    "        # Else: perfectly restored\n",
    "        # Penalize for distance only\n",
    "        R[s][a] = -(1+x+y)\n",
    "    print(\"Progress: \" + str(a+1) + \"/\" + str(A))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learn policy by Q-Learning, Value Iteration, Policy Iteration and Policy Iteration GS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "mdp = mdptoolbox.mdp.QLearning(P, R, 0.99)\n",
    "mdp.run()\n",
    "\n",
    "mdp_policy_iter = mdptoolbox.mdp.PolicyIteration(P, R, .99)\n",
    "mdp_policy_iter.run()\n",
    "\n",
    "mdp_value_iter = mdptoolbox.mdp.ValueIteration(P, R, .99)\n",
    "mdp_value_iter.run()\n",
    "\n",
    "mpd_value_iter_gs = mdptoolbox.mdp.ValueIterationGS(P, R, .99)\n",
    "mpd_value_iter_gs.run()\n",
    "\n",
    "mdp_rel_value_iter = mdptoolbox.mdp.RelativeValueIteration(P, R)\n",
    "mdp_rel_value_iter.run()\n",
    "\n",
    "mdp_policy_iter_modified = mdptoolbox.mdp.PolicyIterationModified(P, R, 0.99)\n",
    "mdp_policy_iter_modified.run()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Greedy policy generation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "greedy_policy = np.ndarray((S,), dtype=int)\n",
    "for s in range(S):\n",
    "    current_max_reward = np.NINF\n",
    "    current_best_action = None\n",
    "    for a in range(A):\n",
    "        if R[s][a] > current_max_reward:\n",
    "            current_max_reward = R[s][a]\n",
    "            current_best_action = a\n",
    "    greedy_policy[s] = current_best_action\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation:\n",
    "Evaluation method for policies using random starting state, stochastic state transitions and calculating average reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def eval_policy(policy, iterations):\n",
    "    # random starting point\n",
    "    # current_state = np.random.randint(0, S)\n",
    "    # Basic starting point: Store white, storage is empty\n",
    "    current_state = 0b001_11_11_11_11\n",
    "    acc_reward = 0\n",
    "    for i in range(iterations):\n",
    "        action = policy[current_state]\n",
    "        acc_reward += R[current_state][action]\n",
    "        states = np.arange(0, S, dtype=int)\n",
    "        current_state = np.random.choice(states, 1, p=P[action][current_state])[0]\n",
    "\n",
    "    return acc_reward / iterations\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy AVG Reward: -10.0\n",
      "Q-Learned MDP AVG Reward: -39.45068\n",
      "Policy Iteration AVG Reward: -7.73803\n",
      "Value Iteration AVG Reward: -7.7554\n",
      "Value Iteration GS AVG Reward: -7.732\n",
      "Relative Value Iteration AVG Reward: -7.73857\n",
      "Policy Iteration Mofified AVG Reward: -7.74901\n"
     ]
    }
   ],
   "source": [
    "print(\"Greedy AVG Reward: \" + str(eval_policy(greedy_policy, 100_000)))\n",
    "print(\"Q-Learned MDP AVG Reward: \" + str(eval_policy(mdp.policy, 100_000)))\n",
    "print(\"Policy Iteration AVG Reward: \" + str(eval_policy(mdp_policy_iter.policy, 100_000)))\n",
    "print(\"Value Iteration AVG Reward: \" + str(eval_policy(mdp_value_iter.policy, 100_000)))\n",
    "print(\"Value Iteration GS AVG Reward: \" + str(eval_policy(mpd_value_iter_gs.policy, 100_000)))\n",
    "print(\"Relative Value Iteration AVG Reward: \" + str(eval_policy(mdp_rel_value_iter.policy, 100_000)))\n",
    "print(\"Policy Iteration Mofified AVG Reward: \" + str(eval_policy(mdp_policy_iter_modified.policy, 100_000)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}